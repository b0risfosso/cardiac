{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e00f918-032b-4724-bc4d-5e11173b142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify phase 0, 1, 2, 3, 4 at cardiac action potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27bd91c-03e4-4e83-a1df-0a94e6ab0485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wfdb\n",
      "  Using cached wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting SoundFile>=0.10.0 (from wfdb)\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/homebrew/lib/python3.11/site-packages (from wfdb) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.10.1 in /opt/homebrew/lib/python3.11/site-packages (from wfdb) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from wfdb) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.8.1 in /opt/homebrew/lib/python3.11/site-packages (from wfdb) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from wfdb) (1.11.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.3.0->wfdb) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.3.0->wfdb) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.8.1->wfdb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.8.1->wfdb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.8.1->wfdb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.8.1->wfdb) (2023.7.22)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from SoundFile>=0.10.0->wfdb) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
      "Using cached wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Installing collected packages: SoundFile, wfdb\n",
      "Successfully installed SoundFile-0.12.1 wfdb-4.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install the wfdb package using pip\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wfdb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7772718f-08b7-4462-8d48-840ba7aa13f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyabf in /opt/homebrew/lib/python3.11/site-packages (2.3.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from pyabf) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from pyabf) (1.26.0)\n",
      "Requirement already satisfied: pytest>=3.0.7 in /opt/homebrew/lib/python3.11/site-packages (from pyabf) (8.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=2.1.0->pyabf) (2.8.2)\n",
      "Requirement already satisfied: iniconfig in /opt/homebrew/lib/python3.11/site-packages (from pytest>=3.0.7->pyabf) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from pytest>=3.0.7->pyabf) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pyabf) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install the pyABF package using pip\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyabf\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8729b-ba0b-4751-9aa6-97613036a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### BUILD THE DATASET ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f33f074e-13ed-403d-882f-fd07893218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot phases based on given start and end times\n",
    "def plot_phases(sweepX, sweepY, phase_times):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Define colors for each phase\n",
    "    colors = {\n",
    "        0: \"red\",\n",
    "        1: \"orange\",\n",
    "        2: \"yellow\",\n",
    "        3: \"green\",\n",
    "        4: \"blue\"\n",
    "    }\n",
    "    \n",
    "    # Plot the original signal\n",
    "    plt.plot(sweepX, sweepY, label=\"Original Signal\", color='black')\n",
    "    \n",
    "    # Plot each phase with a distinct color\n",
    "    for phase, (start_time, end_time) in enumerate(phase_times):\n",
    "        phase_indices = np.where((sweepX >= start_time) & (sweepX <= end_time))[0]\n",
    "        plt.plot(sweepX[phase_indices], sweepY[phase_indices], '.', color=colors[phase], label=f\"Phase {phase}\")\n",
    "    \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (pA or mV)\")\n",
    "    plt.title(\"Action Potential Phases\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "00b160de-505d-45d8-8d5a-a3aed7fcf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define save function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def save_to_csv(sweepX, sweepY, phase_times, csv_file_path, random_sweep_index):\n",
    "    # Save phase times and derivatives to CSV\n",
    "    file_exists = os.path.isfile(csv_file_path)\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['File Path', 'Phase', 'Start Time (s)', 'End Time (s)', 'X Values', 'Y Values', 'Sweep Index'])\n",
    "        for phase, (start_time, end_time) in enumerate(phase_times):\n",
    "            phase_indices = np.where((sweepX >= start_time) & (sweepX <= end_time))[0]\n",
    "            phase_x_values = sweepX[phase_indices].tolist()\n",
    "            phase_y_values = sweepY[phase_indices].tolist()\n",
    "            writer.writerow([file_path, phase, start_time, end_time, phase_x_values, phase_y_values, random_sweep_index])\n",
    "    \n",
    "    print(f\"Phase times and x,y saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "7bd025de-0e0a-45c7-9a77-cc0b7a93abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly chosen filename: sub-2-6-2020/sam-2020-02-06-0004-CF/2020_02_06_0004_CF.abf\n",
      "Randomly chosen sweep index: 184\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pyabf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# read the file\n",
    "# Define the path to the Excel file\n",
    "excel_file_path = \"/Users/b/bio/cardiac/Pennsieve-dataset-297-version-1/files/primary/manifest.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Select the \"filename\" column\n",
    "filenames = df['filename']\n",
    "\n",
    "# Choose a random filename\n",
    "random_filename = random.choice(filenames)\n",
    "\n",
    "file_path = \"/Users/b/bio/cardiac/Pennsieve-dataset-297-version-1/files/primary/\" + random_filename\n",
    "print(f\"Randomly chosen filename: {random_filename}\")\n",
    "\n",
    "# Load the ABF file\n",
    "abf = pyabf.ABF(file_path)\n",
    "\n",
    "# Get the total number of sweeps\n",
    "sweep_count = abf.sweepCount\n",
    "\n",
    "# Choose a random sweep index\n",
    "random_sweep_index = random.randint(0, sweep_count - 1)\n",
    "print(f\"Randomly chosen sweep index: {random_sweep_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "a3297ca5-4ead-475c-a88f-ed52b503d3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Analyze and get the sweep data\n",
    "abf.setSweep(random_sweep_index)\n",
    "sweepX = abf.sweepX\n",
    "sweepY = abf.sweepY\n",
    "\n",
    "\n",
    "# Define the phase times (start_time, end_time) for each phase\n",
    "phase_times = [\n",
    "    (0.02, 0.036),  # Phase 0\n",
    "    (0.036, 0.08),  # Phase 1\n",
    "    (0.08, 0.18),  # Phase 2\n",
    "    (0.18, 0.25),  # Phase 3\n",
    "    (0.25, 2.0)   # Phase 4\n",
    "]\n",
    "\n",
    "# Plot the phases\n",
    "plot_phases(sweepX, sweepY, phase_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "4649033e-b7e0-4e95-a44e-770138e5454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase times and x,y saved to /Users/b/bio/cardiac/phases.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the CSV file path to save the phase times\n",
    "csv_file_path = \"/Users/b/bio/cardiac/phases.csv\"\n",
    "\n",
    "#save to CSV\n",
    "save_to_csv(sweepX, sweepY, phase_times, csv_file_path, random_sweep_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "5df375a4-7882-4c75-b27c-6cb853642d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f3e08-6cd2-44b9-95a3-ef1cda5365cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier will read in a sweep and output the domain for each of the phases. \n",
    "# train a single classifier to output the phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "0553d21f-8a58-44a2-a89f-1fd4ce31d63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                              File Path  Phase  Start Time (s)  \\\n",
       "0    /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      0           0.025   \n",
       "1    /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      1           0.035   \n",
       "2    /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      2           0.100   \n",
       "3    /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      3           0.300   \n",
       "4    /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      4           0.450   \n",
       "..                                                 ...    ...             ...   \n",
       "110  /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      0           0.020   \n",
       "111  /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      1           0.036   \n",
       "112  /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      2           0.080   \n",
       "113  /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      3           0.180   \n",
       "114  /Users/b/bio/cardiac/Pennsieve-dataset-297-ver...      4           0.250   \n",
       "\n",
       "     End Time (s)                                           X Values  \\\n",
       "0           0.035  [0.025, 0.0251, 0.0252, 0.0253, 0.025400000000...   \n",
       "1           0.100  [0.035, 0.0351, 0.0352, 0.035300000000000005, ...   \n",
       "2           0.300  [0.1, 0.10010000000000001, 0.10020000000000001...   \n",
       "3           0.450  [0.3, 0.30010000000000003, 0.3002, 0.3003, 0.3...   \n",
       "4           2.000  [0.45, 0.4501, 0.45020000000000004, 0.45030000...   \n",
       "..            ...                                                ...   \n",
       "110         0.036  [0.02, 0.020050000000000002, 0.0201, 0.02015, ...   \n",
       "111         0.080  [0.036000000000000004, 0.03605, 0.0361, 0.0361...   \n",
       "112         0.180  [0.08, 0.08005000000000001, 0.0801, 0.08015, 0...   \n",
       "113         0.250  [0.18000000000000002, 0.18005000000000002, 0.1...   \n",
       "114         2.000  [0.25, 0.25005, 0.2501, 0.25015000000000004, 0...   \n",
       "\n",
       "                                              Y Values  Sweep Number  \n",
       "0    [-82.122802734375, -82.122802734375, -82.09228...             0  \n",
       "1    [54.01611328125, 53.863525390625, 53.649902343...             0  \n",
       "2    [37.200927734375, 37.200927734375, 37.17041015...             0  \n",
       "3    [9.613037109375, 9.613037109375, 9.61303710937...             0  \n",
       "4    [-79.071044921875, -79.071044921875, -79.10156...             0  \n",
       "..                                                 ...           ...  \n",
       "110  [-79.40673828125, -79.437255859375, -79.437255...           184  \n",
       "111  [42.510986328125, 42.327880859375, 42.20581054...           184  \n",
       "112  [22.3388671875, 22.3388671875, 22.369384765625...           184  \n",
       "113  [-1.5869140625, -1.64794921875, -1.55639648437...           184  \n",
       "114  [-77.63671875, -77.667236328125, -77.697753906...           184  \n",
       "\n",
       "[115 rows x 7 columns]>"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/b/bio/cardiac/model1/phases.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "ec81fdce-6727-4086-9a5a-4a72ead29ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "\n",
    "# read file, sweep number, phase number\n",
    "index = 0\n",
    "file = df['File Path'][index]\n",
    "sweepNum = df['Sweep Number'][index]\n",
    "phaseNum = df['Phase'][index]\n",
    "abf = pyabf.ABF(file)\n",
    "abf.setSweep(sweepNum)\n",
    "\n",
    "# Extract sweepX and sweepY\n",
    "sweepX = abf.sweepX\n",
    "sweepY = abf.sweepY\n",
    "\n",
    "# Create a list of tuples (x, y)\n",
    "totalSweep = list(zip(sweepX, sweepY))\n",
    "\n",
    "phase0_x = df['X Values'][index]\n",
    "phase0_y = df['Y Values'][index]\n",
    "phase0 = list(zip(phase0_x, phase0_y))\n",
    "\n",
    "phase1_x = df['X Values'][index+1]\n",
    "phase1_y = df['Y Values'][index+1]\n",
    "phase1 = list(zip(phase1_x, phase1_y))\n",
    "\n",
    "phase2_x = df['X Values'][index+2]\n",
    "phase2_y = df['Y Values'][index+2]\n",
    "phase2 = list(zip(phase2_x, phase2_y))\n",
    "\n",
    "phase3_x = df['X Values'][index+3]\n",
    "phase3_y = df['Y Values'][index+3]\n",
    "phase3 = list(zip(phase3_x, phase3_y))\n",
    "\n",
    "phase4_x = df['X Values'][index+4]\n",
    "phase4_y = df['Y Values'][index+4]\n",
    "phase4 = list(zip(phase4_x, phase4_y))\n",
    "\n",
    "dataset[str(totalSweep)] = [phase0, phase1, phase2, phase3, phase4]\n",
    "\n",
    "index+=5\n",
    "\n",
    "print(df['Phase'][index])\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "9b846168-8dba-454d-bcfc-4c79c23e6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "input = []\n",
    "index = 0\n",
    "while index < len(df):\n",
    "    file = df['File Path'][index]\n",
    "    sweepNum = df['Sweep Number'][index]\n",
    "    phaseNum = df['Phase'][index]\n",
    "    abf = pyabf.ABF(file)\n",
    "    abf.setSweep(sweepNum)\n",
    "    \n",
    "    # Extract sweepX and sweepY\n",
    "    sweepX = abf.sweepX\n",
    "    sweepY = abf.sweepY\n",
    "    \n",
    "    # Create a list of tuples (x, y)\n",
    "    totalSweep = list(zip(sweepX, sweepY))\n",
    "\n",
    "    input.append(totalSweep)\n",
    "\n",
    "    \n",
    "    phase0_x = ast.literal_eval(df['X Values'][index])\n",
    "    phase0_y = ast.literal_eval(df['Y Values'][index])\n",
    "    phase0 = list(zip(phase0_x, phase0_y))\n",
    "    \n",
    "    \n",
    "    phase1_x = ast.literal_eval(df['X Values'][index+1])\n",
    "    phase1_y = ast.literal_eval(df['Y Values'][index+1])\n",
    "    phase1 = list(zip(phase1_x, phase1_y))\n",
    "    \n",
    "    phase2_x = ast.literal_eval(df['X Values'][index+2])\n",
    "    phase2_y = ast.literal_eval(df['Y Values'][index+2])\n",
    "    phase2 = list(zip(phase2_x, phase2_y))\n",
    "    \n",
    "    phase3_x = ast.literal_eval(df['X Values'][index+3])\n",
    "    phase3_y = ast.literal_eval(df['Y Values'][index+3])\n",
    "    phase3 = list(zip(phase3_x, phase3_y))\n",
    "    \n",
    "    phase4_x = ast.literal_eval(df['X Values'][index+4])\n",
    "    phase4_y = ast.literal_eval(df['Y Values'][index+4])\n",
    "    phase4 = list(zip(phase4_x, phase4_y))\n",
    "\n",
    "    output.append([phase0[-1], phase1[-1], phase2[-1], phase3[-1], phase4[-1]])\n",
    "\n",
    "    index += 5\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "0f5149f0-2c73-456d-a4ac-fb79ca06c3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (1.59.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow)\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow)\n",
      "  Using cached keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (1.26.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: rich in /opt/homebrew/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow)\n",
      "  Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl (236.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.2/236.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Downloading ml_dtypes-0.4.0-cp311-cp311-macosx_10_9_universal2.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.9/390.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, wrapt, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 ml-dtypes-0.4.0 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install the wfdb package using pip\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "38192a17-8f9c-4a9d-9c4c-baa552951b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "f72f2086-51bf-4f5f-af07-99da4c261e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(input)):\n",
    "    dic = {\"1\":input[i],\n",
    "           \"2\":output[i]}\n",
    "    train_data.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "c3a420a9-e580-4443-b598-f13b3ebce406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.035, 54.01611328125),\n",
       " (0.1, 37.200927734375),\n",
       " (0.3, 9.613037109375),\n",
       " (0.45, -79.071044921875),\n",
       " (1.9999, -82.51953125)]"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "eadad7a1-8a20-430d-b6d9-15d0ace4ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(1,2)', '(3,4)', '(5,6)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_data_points(data_points):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of data points in a 2D graph.\n",
    "    \n",
    "    Args:\n",
    "        data_points (list of tuples): A list where each element is a tuple (x, y) representing a data point.\n",
    "        \n",
    "    Returns:\n",
    "        list of str: A list of tokenized strings.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for point in data_points:\n",
    "        x, y = point\n",
    "        tokens.append(f\"({x},{y})\")\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "data_points = [(1, 2), (3, 4), (5, 6)]\n",
    "tokens = tokenize_data_points(data_points)\n",
    "print(tokens)\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i][\"1_tokens\"] = [\"<sos>\"] + tokenize_data_points(train_data[i][\"1\"]) + [\"<eos>\"]\n",
    "    train_data[i][\"2_tokens\"] = [\"<sos>\"] + tokenize_data_points(train_data[i][\"2\"]) + [\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "d44dee17-963b-4738-88e1-6d8a2d572734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.13 requires torch<2.2,>=1.10, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-24.1.2\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "c47078e4-00bc-4bd0-a0a7-0ac175bd9037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp311-none-macosx_11_0_arm64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Using cached torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed torch-2.0.1\n",
      "Collecting torchtext==0.15.2\n",
      "  Downloading torchtext-0.15.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from torchtext==0.15.2) (4.66.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchtext==0.15.2) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /opt/homebrew/lib/python3.11/site-packages (from torchtext==0.15.2) (2.0.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchtext==0.15.2) (1.26.0)\n",
      "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
      "  Downloading torchdata-0.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1->torchtext==0.15.2) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1->torchtext==0.15.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/homebrew/lib/python3.11/site-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext==0.15.2) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext==0.15.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext==0.15.2) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
      "Downloading torchtext-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchdata, torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.18.0\n",
      "    Uninstalling torchtext-0.18.0:\n",
      "      Successfully uninstalled torchtext-0.18.0\n",
      "Successfully installed torchdata-0.6.1 torchtext-0.15.2\n",
      "Requirement already satisfied: spacy in /opt/homebrew/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: evaluate in /opt/homebrew/lib/python3.11/site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (1.26.0)\n",
      "Requirement already satisfied: dill in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (1.26.0)\n",
      "All packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        \"torch==2.0.1\",\n",
    "        \"torchtext==0.15.2\",  # Ensure this version is compatible with torch 2.0.1\n",
    "        \"spacy\",\n",
    "        \"datasets\",\n",
    "        \"tqdm\",\n",
    "        \"evaluate\",\n",
    "        \"numpy\"  # Adding numpy for completeness\n",
    "    ]\n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install_packages()\n",
    "\n",
    "    # Importing the packages to verify installation\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import spacy\n",
    "    import datasets\n",
    "    import torchtext\n",
    "    import tqdm\n",
    "    import evaluate\n",
    "\n",
    "    print(\"All packages installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "ab426d61-f999-45be-8178-44aba0b06329",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tokens = []\n",
    "out_tokens = []\n",
    "for data in train_data:\n",
    "    in_tokens.extend(data[\"1_tokens\"])\n",
    "    out_tokens.extend(data[\"2_tokens\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "e1b67f61-f611-4f8f-ac0f-11afef280453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in tokens\n",
    "unique_in_tokens = remove_duplicates(in_tokens)\n",
    "unique_out_tokens = remove_duplicates(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "24367e69-94e6-486c-8e14-705f84a5a4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523242\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "one_vocab = build_vocab_from_iterator([unique_in_tokens], min_freq=min_freq, specials=special_tokens)\n",
    "\n",
    "# Check the vocab tokens\n",
    "print(len(one_vocab.get_itos()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "a7cd0f18-9aa9-4856-8a16-510175ecfdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<eos>', '<pad>', '<unk>', '(0.01795,77.056884765625)', '(0.01795,92.987060546875)', '(0.02,41.32080078125)', '(0.02,48.309326171875)', '(0.02,50.29296875)', '(0.034,45.135498046875)', '(0.034,50.59814453125)', '(0.034,56.396484375)', '(0.0345,55.2978515625)', '(0.035,33.233642578125)', '(0.035,41.900634765625)', '(0.035,47.454833984375)', '(0.035,51.544189453125)', '(0.035,53.64990234375)', '(0.035,54.01611328125)', '(0.035,54.6875)', '(0.0359,52.79541015625)', '(0.0359,52.947998046875)', '(0.03595,42.66357421875)', '(0.03595,46.417236328125)', '(0.038,46.2646484375)', '(0.04,16.66259765625)', '(0.041,25.08544921875)', '(0.0449,38.543701171875)', '(0.044950000000000004,13.18359375)', '(0.05,31.219482421875)', '(0.055,12.5732421875)', '(0.0599,16.021728515625)', '(0.0599,38.87939453125)', '(0.05995,-52.154541015625)', '(0.05995,32.012939453125)', '(0.07,25.634765625)', '(0.08,22.27783203125)', '(0.08,22.3388671875)', '(0.08,25.81787109375)', '(0.08,28.6865234375)', '(0.08,29.754638671875)', '(0.08,37.078857421875)', '(0.08,38.055419921875)', '(0.08995,27.374267578125)', '(0.08995,34.393310546875)', '(0.1,-80.291748046875)', '(0.1,37.200927734375)', '(0.11,-2.9296875)', '(0.11,32.16552734375)', '(0.13,-12.87841796875)', '(0.14,-2.0751953125)', '(0.14,12.298583984375)', '(0.16,-1.3427734375)', '(0.16,-15.68603515625)', '(0.17995,-1.556396484375)', '(0.19,-77.484130859375)', '(0.2,-82.366943359375)', '(0.2,1.64794921875)', '(0.2,1.861572265625)', '(0.2099,3.23486328125)', '(0.20995,-78.125)', '(0.20995,0.54931640625)', '(0.23,-78.61328125)', '(0.23,-79.742431640625)', '(0.23,11.383056640625)', '(0.25,-70.709228515625)', '(0.25,-77.63671875)', '(0.26,-0.335693359375)', '(0.26,-71.59423828125)', '(0.27,9.368896484375)', '(0.28,-46.44775390625)', '(0.28,-77.606201171875)', '(0.28,17.822265625)', '(0.3,-78.7353515625)', '(0.3,9.613037109375)', '(0.31,-73.66943359375)', '(0.31,-77.545166015625)', '(0.31,-80.2001953125)', '(0.31,-80.810546875)', '(0.32,5.584716796875)', '(0.34990000000000004,-78.277587890625)', '(0.34995000000000004,7.4462890625)', '(0.39,-78.4912109375)', '(0.41995000000000005,-77.880859375)', '(0.45,-79.071044921875)', '(0.45,-79.315185546875)', '(0.45,-85.75439453125)', '(0.47995000000000004,-18.12744140625)', '(0.53,-77.545166015625)', '(0.99995,-73.516845703125)', '(0.99995,-77.0263671875)', '(0.99995,-78.094482421875)', '(0.99995,-79.58984375)', '(0.99995,-80.26123046875)', '(1.9999,-73.79150390625)', '(1.9999,-79.620361328125)', '(1.9999,-79.986572265625)', '(1.9999,-80.047607421875)', '(1.9999,-80.108642578125)', '(1.9999,-80.291748046875)', '(1.9999,-80.413818359375)', '(1.9999,-80.56640625)', '(1.9999,-81.54296875)', '(1.9999,-82.51953125)', '(1.9999500000000001,-79.498291015625)', '(1.9999500000000001,-80.596923828125)', '(1.9999500000000001,-81.84814453125)', '(1.9999500000000001,-82.82470703125)', '(1.9999500000000001,-84.1064453125)', '(1.9999500000000001,-87.799072265625)']\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "two_vocab = build_vocab_from_iterator([unique_out_tokens], min_freq=min_freq, specials=special_tokens)\n",
    "\n",
    "# Check the vocab tokens\n",
    "print(two_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "3ba775ff-961e-46f1-b423-4a3579228535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unk1_index = one_vocab[unk_token]\n",
    "pad1_index = one_vocab[pad_token]\n",
    "\n",
    "unk2_index = two_vocab[unk_token]\n",
    "pad2_index = two_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "15fca36c-5c90-4513-ac28-86cae7f40525",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert one_vocab[unk_token] == two_vocab[unk_token]\n",
    "assert one_vocab[pad_token] == two_vocab[pad_token]\n",
    "\n",
    "unk_index = one_vocab[unk_token]\n",
    "pad_index = one_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "4145a643-ec2c-421b-a711-4c0767666107",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vocab.set_default_index(unk_index)\n",
    "two_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "20302c7d-bd05-47f1-8fa9-c2acaea29092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_vocab[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "023c4cc3-a823-418d-9a58-84ca72d3b3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_vocab.get_itos()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "c9e739a9-167d-40d4-8d23-d589757f9876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<eos>', '<pad>', '<unk>', '(0.035,54.01611328125)', '(0.1,37.200927734375)', '(0.3,9.613037109375)', '(0.45,-79.071044921875)', '(1.9999,-82.51953125)']\n"
     ]
    }
   ],
   "source": [
    "print(one_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "0ccb74e1-91b0-4d04-a966-a2f97538640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, one_vocab, two_vocab):\n",
    "    one_ids = one_vocab.lookup_indices(example[\"1_tokens\"])\n",
    "    two_ids = two_vocab.lookup_indices(example[\"2_tokens\"])\n",
    "    return {\"one_ids\": one_ids, \"two_ids\": two_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "17e686c0-6658-445c-848f-2ec28190db1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[736], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fn_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: one_vocab, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: two_vocab}\n\u001b[0;32m----> 3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(numericalize_example, fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"one_vocab\": one_vocab, \"two_vocab\": two_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "#valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "#test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "8e113b12-bb53-44b5-899d-8bee4f704727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0,     22,     42,  ..., 523199, 523221,      1])\n"
     ]
    }
   ],
   "source": [
    "# Example function to numericalize example and convert to tensors\n",
    "def numericalize_example(example, one_vocab, two_vocab):\n",
    "    example['1_ids'] = torch.tensor([one_vocab[token] for token in example['1_tokens']], dtype=torch.long)\n",
    "    example['2_ids'] = torch.tensor([two_vocab[token] for token in example['2_tokens']], dtype=torch.long)\n",
    "    return example\n",
    "\n",
    "# Apply numericalize_example to each element in train_data\n",
    "fn_kwargs = {\"one_vocab\": one_vocab, \"two_vocab\": two_vocab}\n",
    "train_data = [numericalize_example(data, **fn_kwargs) for data in train_data]\n",
    "\n",
    "# Print the processed train_data\n",
    "print(train_data[0]['1_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "640f1d87-209b-42c8-845c-35d6fed6c146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0]['1_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "da5989c0-e039-406f-88b6-7f30c77839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"1_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"2_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"1_ids\": batch_en_ids,\n",
    "            \"2_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "1c8ddf28-fede-4d51-8f06-2fcb73ccab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "f0fd0f20-2e04-49dd-bb38-ef3f6a6c71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "#valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "#test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "a81e1e59-dc3b-48e0-a91d-82ff62df2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "d1809639-ca91-406c-ad89-0cc887b91831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "3d0c40aa-8171-43cc-804d-e7fddfd7360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build seq2seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "87a7d483-485d-4401-9f06-69b810247170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(one_vocab)\n",
    "output_dim = len(two_vocab)\n",
    "encoder_embedding_dim = 64\n",
    "decoder_embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "9468bd1d-345e-40aa-a63d-b5468bcd6c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(523242, 64)\n",
       "    (rnn): LSTM(64, 128, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(110, 64)\n",
       "    (rnn): LSTM(64, 128, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=128, out_features=110, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "fd47d3d5-5874-435e-a9a6-0df492b667b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 33,971,566 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "5bea2dca-364a-47ab-b538-de865f7f0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "12d2e9dd-de56-4de8-9adf-84a005690e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "9d1ba438-6752-4189-981f-37e0da28ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"2_ids\"].to(device)\n",
    "        trg = batch[\"1_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "d6bcdd9d-92a7-4e43-b8c1-dd8152cad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"2_ids\"].to(device)\n",
    "            trg = batch[\"1_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f9c94-ad74-4760-888e-af51c84c7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "b79e7633-22b0-4cd9-b3ae-4d135ea3529f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[777], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[775], line 12\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# src = [src length, batch size]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# trg = [trg length, batch size]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# output = [trg length, batch size, trg vocab size]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[762], line 35\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# input = [batch size]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_length):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# insert input token embedding, previous hidden and previous cell states\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# receive output tensor (predictions) and new hidden and cell states\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     output, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# output = [batch size, output dim]\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# hidden = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# cell = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[761], line 22\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# input = [1, batch size]\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# embedded = [1, batch size, embedding dim]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded, (hidden, cell))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "7d3eca1c-5355-4cc2-8ae6-e3e13312a0cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 7, 128), got [2, 20002, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[778], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 61\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[778], line 35\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m trg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[762], line 35\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# input = [batch size]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_length):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# insert input token embedding, previous hidden and previous cell states\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# receive output tensor (predictions) and new hidden and cell states\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     output, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# output = [batch size, output dim]\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# hidden = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# cell = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[761], line 24\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     22\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# embedded = [1, batch size, embedding dim]\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# output = [seq length, batch size, hidden dim * n directions]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# hidden = [n layers * n directions, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# cell = [n layers * n directions, batch size, hidden dim]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# hidden = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# cell = [n layers, batch size, hidden dim]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:731\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:239\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    237\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 7, 128), got [2, 20002, 128]"
     ]
    }
   ],
   "source": [
    "encoder_embedding_dim = 64\n",
    "decoder_embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Example train function\n",
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in data_loader:\n",
    "        src = batch['input_tokens'].to(device)\n",
    "        trg = batch['output_tokens'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "# Dummy DataLoader for example purposes\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "\n",
    "# Example optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example training loop\n",
    "n_epochs = 10\n",
    "clip = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
